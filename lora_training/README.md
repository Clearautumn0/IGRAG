# LoRA å¾®è°ƒæ¨¡å—å¿«é€Ÿå¼€å§‹æŒ‡å—

## ğŸ“– æ¨¡å—ç®€ä»‹

`lora_training` æ¨¡å—ä¸º IGRAG ç³»ç»Ÿæä¾› LoRAï¼ˆLow-Rank Adaptationï¼‰å¾®è°ƒåŠŸèƒ½ï¼Œé€šè¿‡å°‘é‡å‚æ•°è®­ç»ƒæå‡ FLAN-T5 æ¨¡å‹åœ¨å›¾åƒæè¿°ç”Ÿæˆä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚

**æ ¸å¿ƒä¼˜åŠ¿**ï¼š
- ğŸ¯ é’ˆå¯¹ COCO é£æ ¼æè¿°ä¼˜åŒ–ï¼ŒBLEU-4 å¯æå‡è‡³ 0.25-0.35
- ğŸ’¡ ä»…è®­ç»ƒå°‘é‡å‚æ•°ï¼ˆLoRA é€‚é…å™¨ï¼‰ï¼Œä¿æŒæ¨¡å‹è½»é‡åŒ–
- ğŸ”„ æ— ç¼é›†æˆåˆ°ç°æœ‰ IGRAG ç³»ç»Ÿï¼Œæ— éœ€ä¿®æ”¹æ¨ç†ä»£ç 

---

## ğŸš€ å¿«é€Ÿå¼€å§‹ï¼ˆæ¨èæ–¹å¼ï¼šä½¿ç”¨è®­ç»ƒè„šæœ¬ï¼‰

### æ–¹å¼ä¸€ï¼šä½¿ç”¨è®­ç»ƒè„šæœ¬ï¼ˆæ¨èï¼‰âœ¨

**å®Œæ•´æµç¨‹ï¼ˆæ„å»ºæ•°æ® + è®­ç»ƒï¼‰**ï¼š

```bash
# ä»é¡¹ç›®æ ¹ç›®å½•è¿è¡Œ
python3 lora_training/train_lora.py --all
```

**ä»…æ„å»ºè®­ç»ƒæ•°æ®**ï¼š

```bash
python3 lora_training/train_lora.py --build-data --sample-count 5000
```

**ä»…è®­ç»ƒæ¨¡å‹**ï¼ˆéœ€è¦å·²æœ‰è®­ç»ƒæ•°æ®ï¼‰ï¼š

```bash
python3 lora_training/train_lora.py --train
```

**è‡ªå®šä¹‰é…ç½®**ï¼š

```bash
python3 lora_training/train_lora.py --all \
    --main-config configs/config.yaml \
    --lora-config lora_training/config/lora_config.yaml \
    --sample-count 5000 \
    --train-ratio 0.9
```

**æŸ¥çœ‹æ‰€æœ‰é€‰é¡¹**ï¼š

```bash
python3 lora_training/train_lora.py --help
```

---

### æ–¹å¼äºŒï¼šä½¿ç”¨ Python API

å¦‚æœä½ æ›´å–œæ¬¢åœ¨ä»£ç ä¸­æ§åˆ¶æµç¨‹ï¼š

**æ­¥éª¤ 1ï¼šæ„å»ºè®­ç»ƒæ•°æ®**

```python
from lora_training.data_builder import LoraTrainingDataBuilder, split_dataset

# æ„å»º 5000 ä¸ªè®­ç»ƒæ ·æœ¬
builder = LoraTrainingDataBuilder(
    main_config_path="configs/config.yaml",
    sample_count=5000,
    output_path="lora_training/data/coco_lora_train.jsonl",
    seed=42
)

# ç”Ÿæˆè®­ç»ƒæ•°æ®
stats = builder.build()
print(f"âœ… ç”Ÿæˆ {stats['num_samples']} ä¸ªæ ·æœ¬ï¼Œä¿å­˜è‡³ {stats['output_path']}")

# è‡ªåŠ¨åˆ‡åˆ†ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†ï¼ˆ9:1ï¼‰
split_dataset(stats["output_path"], train_ratio=0.9, seed=42)
```

**æ­¥éª¤ 2ï¼šé…ç½®å¹¶å¯åŠ¨è®­ç»ƒ**

ç¼–è¾‘ `lora_training/config/lora_config.yaml`ï¼Œç¡®è®¤ä»¥ä¸‹å…³é”®å‚æ•°ï¼š

```yaml
model:
  base_model_path: "../models/flan-t5-large"  # åŸºç¡€æ¨¡å‹è·¯å¾„
  type: "flan-t5"

data:
  train_path: "lora_training/data/coco_lora_train_train.jsonl"
  val_path: "lora_training/data/coco_lora_train_val.jsonl"

training:
  num_train_epochs: 3          # è®­ç»ƒè½®æ•°
  train_batch_size: 4          # æ ¹æ® GPU æ˜¾å­˜è°ƒæ•´
  gradient_accumulation_steps: 8  # æœ‰æ•ˆ batch = 4 Ã— 8 = 32
  learning_rate: 1.0e-4
  output_dir: "lora_training/checkpoints"

lora:
  r: 16                        # LoRA ç§©ï¼ˆå¹³è¡¡æ•ˆæœä¸å‚æ•°é‡ï¼‰
  lora_alpha: 32              # ç¼©æ”¾å‚æ•°ï¼ˆé€šå¸¸ä¸º 2Ã—rï¼‰
  dropout: 0.1
  target_modules: ["q", "v"]  # é’ˆå¯¹æŸ¥è¯¢å’Œå€¼çŸ©é˜µé€‚é…
```

å¯åŠ¨è®­ç»ƒï¼š

```python
from lora_training.lora_trainer import LoraCaptionTrainer

# åˆå§‹åŒ–è®­ç»ƒå™¨ï¼ˆè‡ªåŠ¨åŠ è½½é…ç½®ï¼‰
trainer = LoraCaptionTrainer("lora_training/config/lora_config.yaml")

# å¼€å§‹è®­ç»ƒï¼ˆæ¯ä¸ª epoch è‡ªåŠ¨ä¿å­˜ checkpoint å¹¶è¯„ä¼° BLEUï¼‰
train_result = trainer.train()

# è¿è¡Œæœ€ç»ˆè¯„ä¼°
eval_metrics = trainer.evaluate()
print(f"âœ… æœ€ç»ˆ BLEU åˆ†æ•°: {eval_metrics.get('eval_bleu', 'N/A')}")
```

**è¾“å‡ºæ–‡ä»¶**ï¼š
- `lora_training/data/coco_lora_train_train.jsonl` - è®­ç»ƒé›†ï¼ˆ4500 æ ·æœ¬ï¼‰
- `lora_training/data/coco_lora_train_val.jsonl` - éªŒè¯é›†ï¼ˆ500 æ ·æœ¬ï¼‰
- `lora_training/checkpoints/checkpoint-{step}/` - æ¯ä¸ª epoch çš„æ£€æŸ¥ç‚¹
- `lora_training/checkpoints/best/` - æœ€ä½³æ¨¡å‹ï¼ˆåŸºäºéªŒè¯é›† BLEUï¼‰

---

### æ­¥éª¤ 3ï¼šé›†æˆåˆ° IGRAG ç³»ç»Ÿ

åœ¨ `configs/config.yaml` ä¸­å¯ç”¨ LoRAï¼š

```yaml
lora_config:
  enabled: true
  weights_path: "lora_training/checkpoints/best"  # æœ€ä½³ checkpoint è·¯å¾„
  merge_and_unload: false  # false=åŠ¨æ€åŠ è½½ï¼Œtrue=åˆå¹¶åˆ°åŸºç¡€æ¨¡å‹
```

é‡æ–°è¿è¡Œ IGRAG ç³»ç»Ÿï¼Œ`CaptionGenerator` ä¼šè‡ªåŠ¨åŠ è½½ LoRA é€‚é…å™¨ï¼š

```bash
python3 main.py --i input/802.jpg --model flan-t5
```

---

## ğŸ“ æ–‡ä»¶ç»“æ„

```
lora_training/
â”œâ”€â”€ README.md                    # æœ¬æ–‡æ¡£
â”œâ”€â”€ __init__.py
â”œâ”€â”€ data_builder.py              # è®­ç»ƒæ•°æ®æ„å»ºå™¨
â”œâ”€â”€ lora_trainer.py              # LoRA è®­ç»ƒå™¨ä¸»ç±»
â”œâ”€â”€ config/
â”‚   â””â”€â”€ lora_config.yaml         # è®­ç»ƒé…ç½®æ–‡ä»¶
â””â”€â”€ utils/
    â”œâ”€â”€ __init__.py
    â””â”€â”€ training_utils.py        # æ•°æ®é›†ã€å·¥å…·å‡½æ•°
```

---

## ğŸ”§ æ ¸å¿ƒç»„ä»¶è¯´æ˜

### `data_builder.py`

**`LoraTrainingDataBuilder`**ï¼šä» COCO æ•°æ®é›†æ„å»ºè®­ç»ƒæ ·æœ¬
- ä½¿ç”¨ IGRAG æ£€ç´¢å™¨ç”Ÿæˆ promptï¼ˆåŒ…å«å…¨å±€æè¿° + å±€éƒ¨ä½ç½®ä¿¡æ¯ï¼‰
- ä½¿ç”¨ COCO æ ‡æ³¨ä½œä¸ºç›®æ ‡ caption
- è¾“å‡º JSONL æ ¼å¼ï¼š`{"image_id": ..., "prompt": ..., "caption": ..., "metadata": ...}`

**`split_dataset()`**ï¼šå°†æ•°æ®é›†åˆ‡åˆ†ä¸ºè®­ç»ƒ/éªŒè¯é›†

---

### `lora_trainer.py`

**`LoraCaptionTrainer`**ï¼šç«¯åˆ°ç«¯è®­ç»ƒå…¥å£
- è‡ªåŠ¨åŠ è½½åŸºç¡€æ¨¡å‹ï¼ˆFLAN-T5ï¼‰
- åº”ç”¨ LoRA é…ç½®ï¼ˆPEFTï¼‰
- ä½¿ç”¨ Transformers Trainer è¿›è¡Œè®­ç»ƒ
- è‡ªåŠ¨è®¡ç®—éªŒè¯é›† BLEU åˆ†æ•°
- ä¿å­˜æœ€ä½³ checkpoint

---

### `config/lora_config.yaml`

è®­ç»ƒé…ç½®åˆ†ä¸º 4 ä¸ªéƒ¨åˆ†ï¼š
- **`model`**ï¼šåŸºç¡€æ¨¡å‹è·¯å¾„å’Œç±»å‹
- **`data`**ï¼šè®­ç»ƒ/éªŒè¯æ•°æ®è·¯å¾„å’Œé•¿åº¦é™åˆ¶
- **`training`**ï¼šè®­ç»ƒè¶…å‚æ•°ï¼ˆepochs, batch size, learning rate ç­‰ï¼‰
- **`lora`**ï¼šLoRA ç‰¹å®šå‚æ•°ï¼ˆr, alpha, dropout, target_modulesï¼‰

---

## ğŸ’¡ ä½¿ç”¨ç¤ºä¾‹

### å®Œæ•´è®­ç»ƒæµç¨‹è„šæœ¬

```python
#!/usr/bin/env python3
"""å®Œæ•´çš„ LoRA å¾®è°ƒæµç¨‹"""

from pathlib import Path
from lora_training.data_builder import LoraTrainingDataBuilder, split_dataset
from lora_training.lora_trainer import LoraCaptionTrainer

# === é˜¶æ®µ 1ï¼šæ„å»ºè®­ç»ƒæ•°æ® ===
print("ğŸ“¦ é˜¶æ®µ 1: æ„å»ºè®­ç»ƒæ•°æ®...")
builder = LoraTrainingDataBuilder(
    main_config_path="configs/config.yaml",
    sample_count=5000,
    output_path="lora_training/data/coco_lora_train.jsonl",
    seed=42
)
stats = builder.build()
print(f"âœ… ç”Ÿæˆ {stats['num_samples']} ä¸ªæ ·æœ¬")

# åˆ‡åˆ†æ•°æ®é›†
split_dataset(stats["output_path"], train_ratio=0.9, seed=42)
print("âœ… æ•°æ®é›†åˆ‡åˆ†å®Œæˆ\n")

# === é˜¶æ®µ 2ï¼šè®­ç»ƒ ===
print("ğŸš€ é˜¶æ®µ 2: å¼€å§‹ LoRA è®­ç»ƒ...")
trainer = LoraCaptionTrainer("lora_training/config/lora_config.yaml")

# è®­ç»ƒ 3 ä¸ª epoch
train_result = trainer.train()
print(f"âœ… è®­ç»ƒå®Œæˆï¼ŒæŸå¤±: {train_result.metrics.get('train_loss', 'N/A')}")

# è¯„ä¼°
eval_metrics = trainer.evaluate()
print(f"âœ… éªŒè¯é›† BLEU: {eval_metrics.get('eval_bleu', 'N/A')}\n")

# === é˜¶æ®µ 3ï¼šæç¤ºé›†æˆ ===
print("ğŸ“ é˜¶æ®µ 3: è¯·æ‰‹åŠ¨åœ¨ configs/config.yaml ä¸­å¯ç”¨ LoRA:")
print("""
lora_config:
  enabled: true
  weights_path: "lora_training/checkpoints/best"
  merge_and_unload: false
""")
```

---

## âš™ï¸ å‚æ•°è°ƒä¼˜å»ºè®®

### LoRA å‚æ•°

| å‚æ•° | æ¨èå€¼ | è¯´æ˜ |
|------|--------|------|
| `r` | 16 | ç§©ï¼Œè¶Šå¤§æ•ˆæœè¶Šå¥½ä½†å‚æ•°è¶Šå¤šï¼ˆ8/16/32 å¸¸è§ï¼‰ |
| `lora_alpha` | 32 | é€šå¸¸è®¾ä¸º `2 Ã— r` |
| `dropout` | 0.1 | é˜²æ­¢è¿‡æ‹Ÿåˆï¼ˆ0.05-0.2 èŒƒå›´ï¼‰ |
| `target_modules` | `["q", "v"]` | é’ˆå¯¹æ³¨æ„åŠ›å±‚çš„æŸ¥è¯¢å’Œå€¼çŸ©é˜µ |

### è®­ç»ƒå‚æ•°

| å‚æ•° | æ¨èå€¼ | è¯´æ˜ |
|------|--------|------|
| `num_train_epochs` | 3 | é€šå¸¸ 3-5 ä¸ª epoch è¶³å¤Ÿ |
| `learning_rate` | 1e-4 | å¯å°è¯• 5e-5 åˆ° 2e-4 |
| `train_batch_size` | 4 | æ ¹æ® GPU æ˜¾å­˜è°ƒæ•´ï¼ˆ2-8ï¼‰ |
| `gradient_accumulation_steps` | 8 | æœ‰æ•ˆ batch = batch_size Ã— accumulation |

---

## â“ å¸¸è§é—®é¢˜

### Q1: è®­ç»ƒéœ€è¦å¤šå°‘ GPU æ˜¾å­˜ï¼Ÿ

**A**: ä½¿ç”¨ `train_batch_size=4, gradient_accumulation_steps=8` æ—¶ï¼š
- FLAN-T5-base: çº¦ 6-8 GB
- FLAN-T5-large: çº¦ 12-16 GB

å¦‚æœæ˜¾å­˜ä¸è¶³ï¼Œå‡å° `train_batch_size` æˆ–å¢å¤§ `gradient_accumulation_steps`ã€‚

---

### Q2: å¦‚ä½•é€‰æ‹©æœ€ä½³ checkpointï¼Ÿ

**A**: è®­ç»ƒå™¨ä¼šè‡ªåŠ¨é€‰æ‹©éªŒè¯é›† BLEU æœ€é«˜çš„ checkpoint ä¿å­˜ä¸º `best/`ã€‚ä½ ä¹Ÿå¯ä»¥ï¼š
- æŸ¥çœ‹ `lora_training/checkpoints/` ä¸‹çš„å„ checkpoint
- æ£€æŸ¥è®­ç»ƒæ—¥å¿—ä¸­çš„ `eval_bleu` åˆ†æ•°
- æ‰‹åŠ¨æµ‹è¯•ä¸åŒ checkpoint åœ¨éªŒè¯é›†ä¸Šçš„è¡¨ç°

---

### Q3: `merge_and_unload` é€‰é¡¹çš„ä½œç”¨ï¼Ÿ

**A**: 
- `false`ï¼ˆæ¨èï¼‰ï¼šåŠ¨æ€åŠ è½½ LoRA é€‚é…å™¨ï¼Œä¿æŒåŸºç¡€æ¨¡å‹ä¸å˜ï¼Œå¯çµæ´»åˆ‡æ¢ä¸åŒ LoRA
- `true`ï¼šå°† LoRA æƒé‡åˆå¹¶åˆ°åŸºç¡€æ¨¡å‹å¹¶å¸è½½é€‚é…å™¨ï¼Œç”Ÿæˆæ–°çš„å®Œæ•´æ¨¡å‹ï¼ˆå ç”¨æ›´å¤šç©ºé—´ï¼‰

---

### Q4: è®­ç»ƒæ•°æ®ä¸å¤Ÿæ€ä¹ˆåŠï¼Ÿ

**A**: å¯ä»¥ï¼š
- å¢åŠ  `sample_count`ï¼ˆå¦‚ 10000ï¼‰
- ä½¿ç”¨æ›´å¤š COCO è®­ç»ƒé›†å›¾åƒ
- è°ƒæ•´ `data_builder.py` ä¸­çš„æ£€ç´¢å‚æ•°ä»¥è·å–æ›´ä¸°å¯Œçš„ prompt

---

### Q5: å¦‚ä½•ç›‘æ§è®­ç»ƒè¿‡ç¨‹ï¼Ÿ

**A**: 
- æŸ¥çœ‹æ§åˆ¶å°è¾“å‡ºçš„ `eval_bleu` å’Œ `eval_loss`
- æ£€æŸ¥ `lora_training/checkpoints/training_state.json` ä¸­çš„è®­ç»ƒå†å²
- å¦‚æœé…ç½®äº† `report_to: ["tensorboard"]`ï¼Œå¯ä½¿ç”¨ TensorBoard å¯è§†åŒ–

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

- ä¸»é¡¹ç›® README: `../README.md`ï¼ˆåŒ…å« LoRA é›†æˆè¯´æ˜ï¼‰
- é…ç½®æ–‡ä»¶: `configs/config.yaml`ï¼ˆä¸»ç³»ç»Ÿé…ç½®ï¼‰
- è®­ç»ƒé…ç½®: `config/lora_config.yaml`ï¼ˆLoRA è®­ç»ƒé…ç½®ï¼‰

---

## ğŸ¯ é¢„æœŸæ•ˆæœ

ä½¿ç”¨æ¨èçš„ LoRA é…ç½®ï¼ˆ`r=16, alpha=32`ï¼‰è®­ç»ƒ 3 ä¸ª epoch åï¼š
- **BLEU-4**: ä» 0.22 æå‡è‡³ **0.25-0.35**
- **æè¿°è´¨é‡**: æ›´ç¬¦åˆ COCO é£æ ¼ï¼Œæ›´å‡†ç¡®çš„ç©ºé—´å…³ç³»æè¿°
- **å‚æ•°é‡**: ä»…å¢åŠ çº¦ 1-2% çš„å¯è®­ç»ƒå‚æ•°

---

**ç¥è®­ç»ƒé¡ºåˆ©ï¼** ğŸ‰

