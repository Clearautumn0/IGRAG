#!/usr/bin/env python3
"""LoRA è®­ç»ƒè„šæœ¬ï¼šä»æ•°æ®æ„å»ºåˆ°æ¨¡å‹è®­ç»ƒçš„å®Œæ•´æµç¨‹

ç”¨æ³•:
    # å®Œæ•´æµç¨‹ï¼ˆæ„å»ºæ•°æ® + è®­ç»ƒï¼‰
    python3 lora_training/train_lora.py --all

    # ä»…æ„å»ºè®­ç»ƒæ•°æ®
    python3 lora_training/train_lora.py --build-data --sample-count 5000

    # ä»…è®­ç»ƒï¼ˆéœ€è¦å·²æœ‰è®­ç»ƒæ•°æ®ï¼‰
    python3 lora_training/train_lora.py --train

    # è‡ªå®šä¹‰é…ç½®
    python3 lora_training/train_lora.py --all --lora-config lora_training/config/lora_config.yaml

è¾“å‡º:
    - lora_training/data/coco_lora_train.jsonl (åŸå§‹æ•°æ®)
    - lora_training/data/coco_lora_train_train.jsonl (è®­ç»ƒé›†)
    - lora_training/data/coco_lora_train_val.jsonl (éªŒè¯é›†)
    - lora_training/checkpoints/ (è®­ç»ƒæ£€æŸ¥ç‚¹)
"""
import argparse
import logging
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥æ¨¡å—
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from lora_training.data_builder import LoraTrainingDataBuilder, split_dataset
from lora_training.lora_trainer import LoraCaptionTrainer


def setup_logging(level: str = "INFO"):
    """è®¾ç½®æ—¥å¿—çº§åˆ«"""
    level_map = {
        "DEBUG": logging.DEBUG,
        "INFO": logging.INFO,
        "WARNING": logging.WARNING,
        "ERROR": logging.ERROR,
    }
    logging.basicConfig(
        level=level_map.get(level.upper(), logging.INFO),
        format="%(asctime)s [%(levelname)s] %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
    )


def build_training_data(
    main_config_path: str = "configs/config.yaml",
    sample_count: int = 5000,
    output_path: str = "lora_training/data/coco_lora_train.jsonl",
    train_ratio: float = 0.9,
    seed: int = 42,
    skip_if_exists: bool = True,
) -> dict:
    """æ„å»ºè®­ç»ƒæ•°æ®"""
    print("=" * 70)
    print("é˜¶æ®µ 1: æ„å»º LoRA è®­ç»ƒæ•°æ®")
    print("=" * 70)

    output_path_obj = Path(output_path)
    if skip_if_exists and output_path_obj.exists():
        print(f"âš ï¸  æ•°æ®æ–‡ä»¶å·²å­˜åœ¨: {output_path}")
        response = input("æ˜¯å¦é‡æ–°æ„å»º? (y/N): ").strip().lower()
        if response != "y":
            print("è·³è¿‡æ•°æ®æ„å»ºï¼Œä½¿ç”¨ç°æœ‰æ–‡ä»¶")
            # æ£€æŸ¥æ˜¯å¦å·²æœ‰åˆ‡åˆ†åçš„æ–‡ä»¶
            train_path = output_path_obj.with_name(output_path_obj.stem + "_train.jsonl")
            val_path = output_path_obj.with_name(output_path_obj.stem + "_val.jsonl")
            if train_path.exists() and val_path.exists():
                return {
                    "output_path": str(output_path_obj),
                    "train_path": str(train_path),
                    "val_path": str(val_path),
                    "num_samples": "unknown (using existing)",
                }
            else:
                print("ç°æœ‰æ–‡ä»¶æœªåˆ‡åˆ†ï¼Œè¿›è¡Œåˆ‡åˆ†...")
                split_dataset(str(output_path_obj), train_ratio=train_ratio, seed=seed)
                train_path = output_path_obj.with_name(output_path_obj.stem + "_train.jsonl")
                val_path = output_path_obj.with_name(output_path_obj.stem + "_val.jsonl")
                return {
                    "output_path": str(output_path_obj),
                    "train_path": str(train_path),
                    "val_path": str(val_path),
                    "num_samples": "unknown (using existing)",
                }

    print(f"ğŸ“¦ å¼€å§‹æ„å»º {sample_count} ä¸ªè®­ç»ƒæ ·æœ¬...")
    print(f"   ä¸»é…ç½®æ–‡ä»¶: {main_config_path}")
    print(f"   è¾“å‡ºè·¯å¾„: {output_path}")
    print()

    try:
        builder = LoraTrainingDataBuilder(
            main_config_path=main_config_path,
            sample_count=sample_count,
            output_path=output_path,
            seed=seed,
        )
        stats = builder.build()
        print(f"âœ… æˆåŠŸç”Ÿæˆ {stats['num_samples']} ä¸ªè®­ç»ƒæ ·æœ¬")
        print(f"   ä¿å­˜ä½ç½®: {stats['output_path']}")
        print()

        # è‡ªåŠ¨åˆ‡åˆ†æ•°æ®é›†
        print("ğŸ“Š åˆ‡åˆ†æ•°æ®é›† (è®­ç»ƒé›†/éªŒè¯é›† = {:.0%}/{:.0%})...".format(train_ratio, 1 - train_ratio))
        split_result = split_dataset(stats["output_path"], train_ratio=train_ratio, seed=seed)
        print(f"âœ… æ•°æ®é›†åˆ‡åˆ†å®Œæˆ")
        print(f"   è®­ç»ƒé›†: {split_result['train_path']}")
        print(f"   éªŒè¯é›†: {split_result['val_path']}")
        print()

        return {
            **stats,
            **split_result,
        }
    except Exception as e:
        logging.error(f"æ•°æ®æ„å»ºå¤±è´¥: {e}", exc_info=True)
        raise


def train_lora(
    lora_config_path: str = "lora_training/config/lora_config.yaml",
    train_path: str = None,
    val_path: str = None,
) -> dict:
    """è®­ç»ƒ LoRA æ¨¡å‹"""
    print("=" * 70)
    print("é˜¶æ®µ 2: LoRA æ¨¡å‹è®­ç»ƒ")
    print("=" * 70)

    print(f"ğŸ“‹ é…ç½®æ–‡ä»¶: {lora_config_path}")
    if train_path:
        print(f"   è®­ç»ƒé›†: {train_path}")
    if val_path:
        print(f"   éªŒè¯é›†: {val_path}")
    print()

    try:
        # å¦‚æœæä¾›äº†æ•°æ®è·¯å¾„ï¼Œä¸´æ—¶æ›´æ–°é…ç½®
        if train_path or val_path:
            import yaml
            with open(lora_config_path, "r") as f:
                config = yaml.safe_load(f)
            if train_path:
                config["data"]["train_path"] = train_path
            if val_path:
                config["data"]["val_path"] = val_path
            # ä¿å­˜ä¸´æ—¶é…ç½®
            temp_config_path = lora_config_path.replace(".yaml", "_temp.yaml")
            with open(temp_config_path, "w") as f:
                yaml.dump(config, f)
            lora_config_path = temp_config_path

        trainer = LoraCaptionTrainer(lora_config_path)
        print("âœ… è®­ç»ƒå™¨åˆå§‹åŒ–æˆåŠŸ")
        print()

        # æ˜¾ç¤ºè®­ç»ƒå‚æ•°æ‘˜è¦
        print("ğŸ“Š è®­ç»ƒé…ç½®æ‘˜è¦:")
        print(f"   åŸºç¡€æ¨¡å‹: {trainer.base_model_path}")
        print(f"   è®­ç»ƒè½®æ•°: {trainer.training_cfg.get('num_train_epochs', 3)}")
        print(f"   æ‰¹æ¬¡å¤§å°: {trainer.training_cfg.get('train_batch_size', 4)}")
        print(f"   æ¢¯åº¦ç´¯ç§¯: {trainer.training_cfg.get('gradient_accumulation_steps', 4)}")
        print(f"   å­¦ä¹ ç‡: {trainer.training_cfg.get('learning_rate', 5e-5)}")
        print(f"   LoRA r: {trainer.lora_cfg.get('r', 16)}")
        print(f"   LoRA alpha: {trainer.lora_cfg.get('lora_alpha', 32)}")
        print()

        # å¼€å§‹è®­ç»ƒ
        print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
        print("-" * 70)
        train_result = trainer.train()
        print("-" * 70)
        print("âœ… è®­ç»ƒå®Œæˆ")
        print(f"   æœ€ç»ˆè®­ç»ƒæŸå¤±: {train_result.metrics.get('train_loss', 'N/A'):.4f}")
        print()

        # è¿è¡Œè¯„ä¼°
        print("ğŸ“ˆ è¿è¡ŒéªŒè¯é›†è¯„ä¼°...")
        eval_metrics = trainer.evaluate()
        print("âœ… è¯„ä¼°å®Œæˆ")
        print(f"   éªŒè¯é›† BLEU: {eval_metrics.get('eval_bleu', 'N/A'):.4f}")
        print(f"   éªŒè¯é›†æŸå¤±: {eval_metrics.get('eval_loss', 'N/A'):.4f}")
        print()

        # æ˜¾ç¤ºæœ€ä½³æ¨¡å‹ä½ç½®
        output_dir = trainer.training_cfg.get("output_dir", "lora_training/checkpoints")
        best_model_path = Path(output_dir) / "best"
        if best_model_path.exists():
            print(f"âœ… æœ€ä½³æ¨¡å‹å·²ä¿å­˜è‡³: {best_model_path}")
        else:
            print(f"ğŸ“ æ£€æŸ¥ç‚¹ä¿å­˜åœ¨: {output_dir}")

        # æ¸…ç†ä¸´æ—¶é…ç½®
        if train_path or val_path:
            temp_config = Path(lora_config_path)
            if temp_config.exists():
                temp_config.unlink()

        return {
            "train_metrics": train_result.metrics,
            "eval_metrics": eval_metrics,
            "best_model_path": str(best_model_path) if best_model_path.exists() else output_dir,
        }
    except Exception as e:
        logging.error(f"è®­ç»ƒå¤±è´¥: {e}", exc_info=True)
        raise


def main():
    parser = argparse.ArgumentParser(
        description="LoRA è®­ç»ƒè„šæœ¬ï¼šä»æ•°æ®æ„å»ºåˆ°æ¨¡å‹è®­ç»ƒçš„å®Œæ•´æµç¨‹",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # å®Œæ•´æµç¨‹
  python3 lora_training/train_lora.py --all

  # ä»…æ„å»ºæ•°æ®
  python3 lora_training/train_lora.py --build-data --sample-count 10000

  # ä»…è®­ç»ƒ
  python3 lora_training/train_lora.py --train

  # è‡ªå®šä¹‰é…ç½®
  python3 lora_training/train_lora.py --all \\
      --main-config configs/config.yaml \\
      --lora-config lora_training/config/lora_config.yaml \\
      --sample-count 5000
        """,
    )

    # ä¸»è¦æ“ä½œé€‰é¡¹
    parser.add_argument(
        "--all",
        action="store_true",
        help="æ‰§è¡Œå®Œæ•´æµç¨‹ï¼šæ„å»ºæ•°æ® + è®­ç»ƒ",
    )
    parser.add_argument(
        "--build-data",
        action="store_true",
        help="ä»…æ„å»ºè®­ç»ƒæ•°æ®",
    )
    parser.add_argument(
        "--train",
        action="store_true",
        help="ä»…è®­ç»ƒæ¨¡å‹ï¼ˆéœ€è¦å·²æœ‰è®­ç»ƒæ•°æ®ï¼‰",
    )

    # é…ç½®æ–‡ä»¶
    parser.add_argument(
        "--main-config",
        type=str,
        default="configs/config.yaml",
        help="ä¸»é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆç”¨äºæ•°æ®æ„å»ºï¼‰",
    )
    parser.add_argument(
        "--lora-config",
        type=str,
        default="lora_training/config/lora_config.yaml",
        help="LoRA è®­ç»ƒé…ç½®æ–‡ä»¶è·¯å¾„",
    )

    # æ•°æ®æ„å»ºå‚æ•°
    parser.add_argument(
        "--sample-count",
        type=int,
        default=5000,
        help="è®­ç»ƒæ ·æœ¬æ•°é‡ï¼ˆé»˜è®¤: 5000ï¼‰",
    )
    parser.add_argument(
        "--output-data",
        type=str,
        default="lora_training/data/coco_lora_train.jsonl",
        help="è®­ç»ƒæ•°æ®è¾“å‡ºè·¯å¾„",
    )
    parser.add_argument(
        "--train-ratio",
        type=float,
        default=0.9,
        help="è®­ç»ƒé›†æ¯”ä¾‹ï¼ˆé»˜è®¤: 0.9ï¼Œå³ 90%% è®­ç»ƒï¼Œ10%% éªŒè¯ï¼‰",
    )
    parser.add_argument(
        "--seed",
        type=int,
        default=42,
        help="éšæœºç§å­ï¼ˆé»˜è®¤: 42ï¼‰",
    )
    parser.add_argument(
        "--force-rebuild",
        action="store_true",
        help="å¼ºåˆ¶é‡æ–°æ„å»ºæ•°æ®ï¼ˆå³ä½¿æ–‡ä»¶å·²å­˜åœ¨ï¼‰",
    )

    # è®­ç»ƒæ•°æ®è·¯å¾„ï¼ˆç”¨äºä»…è®­ç»ƒæ¨¡å¼ï¼‰
    parser.add_argument(
        "--train-path",
        type=str,
        help="è®­ç»ƒé›†è·¯å¾„ï¼ˆè¦†ç›–é…ç½®æ–‡ä»¶ä¸­çš„è®¾ç½®ï¼‰",
    )
    parser.add_argument(
        "--val-path",
        type=str,
        help="éªŒè¯é›†è·¯å¾„ï¼ˆè¦†ç›–é…ç½®æ–‡ä»¶ä¸­çš„è®¾ç½®ï¼‰",
    )

    # æ—¥å¿—çº§åˆ«
    parser.add_argument(
        "--log-level",
        type=str,
        default="INFO",
        choices=["DEBUG", "INFO", "WARNING", "ERROR"],
        help="æ—¥å¿—çº§åˆ«ï¼ˆé»˜è®¤: INFOï¼‰",
    )

    args = parser.parse_args()

    # å¦‚æœæ²¡æœ‰æŒ‡å®šä»»ä½•æ“ä½œï¼Œæ˜¾ç¤ºå¸®åŠ©
    if not (args.all or args.build_data or args.train):
        parser.print_help()
        sys.exit(1)

    # è®¾ç½®æ—¥å¿—
    setup_logging(args.log_level)

    try:
        # æ‰§è¡Œæ“ä½œ
        data_stats = None
        train_stats = None

        if args.all or args.build_data:
            data_stats = build_training_data(
                main_config_path=args.main_config,
                sample_count=args.sample_count,
                output_path=args.output_data,
                train_ratio=args.train_ratio,
                seed=args.seed,
                skip_if_exists=not args.force_rebuild,
            )

        if args.all or args.train:
            # å¦‚æœæ•°æ®åˆšæ„å»ºå®Œæˆï¼Œä½¿ç”¨æ–°çš„è·¯å¾„
            train_path = args.train_path
            val_path = args.val_path
            if data_stats:
                train_path = data_stats.get("train_path") or train_path
                val_path = data_stats.get("val_path") or val_path

            train_stats = train_lora(
                lora_config_path=args.lora_config,
                train_path=train_path,
                val_path=val_path,
            )

        # æ€»ç»“
        print("=" * 70)
        print("âœ… æ‰€æœ‰æ“ä½œå®Œæˆ")
        print("=" * 70)
        if data_stats:
            print(f"ğŸ“¦ æ•°æ®: {data_stats.get('num_samples', 'N/A')} ä¸ªæ ·æœ¬")
        if train_stats:
            print(f"ğŸ“ˆ è®­ç»ƒ: BLEU = {train_stats.get('eval_metrics', {}).get('eval_bleu', 'N/A'):.4f}")
            print(f"ğŸ“ æ¨¡å‹: {train_stats.get('best_model_path', 'N/A')}")
        print()
        print("ğŸ’¡ ä¸‹ä¸€æ­¥: åœ¨ configs/config.yaml ä¸­å¯ç”¨ LoRA:")
        print("   lora_config:")
        print("     enabled: true")
        if train_stats:
            print(f"     weights_path: \"{train_stats.get('best_model_path', 'lora_training/checkpoints/best')}\"")
        print("=" * 70)

    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­æ“ä½œ")
        sys.exit(1)
    except Exception as e:
        logging.error(f"æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
        sys.exit(1)


if __name__ == "__main__":
    main()

