model:
  base_model_path: "/home/m025/qqw/models/flan-t5-base"
  type: "flan-t5"

data:
  train_path: "lora_training/data/coco_lora_train_train.jsonl"
  val_path: "lora_training/data/coco_lora_train_val.jsonl"
  max_source_length: 512
  max_target_length: 64

training:
  output_dir: "lora_training/checkpoints"
  num_train_epochs: 3
  train_batch_size: 16
  eval_batch_size: 16
  gradient_accumulation_steps: 8
  learning_rate: 1.0e-4
  weight_decay: 0.01
  warmup_steps: 150
  logging_steps: 25
  eval_strategy: "epoch"  # 新版本 transformers 使用 eval_strategy 而不是 evaluation_strategy
  save_strategy: "epoch"
  save_total_limit: 3
  seed: 42
  fp16: true
  bf16: false
  report_to: []

lora:
  r: 16
  lora_alpha: 32
  dropout: 0.1
  target_modules: ["q", "v"]

