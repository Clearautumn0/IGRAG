#!/usr/bin/env python3
"""æµ‹è¯•BLIP-2å¯†é›†æè¿°ç”ŸæˆåŠŸèƒ½ã€‚

å¤„ç†å°‘é‡å›¾åƒï¼ˆé»˜è®¤10å¼ ï¼‰ä»¥éªŒè¯è¾“å‡ºæ ¼å¼ã€‚
"""
import os
import sys
import json
import pickle
import logging
from pathlib import Path
from typing import List, Dict

import torch
from tqdm import tqdm
import yaml
from PIL import Image

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, project_root)

from scripts.build_dense_knowledge_base import (
    load_config,
    load_coco_image_mapping,
    image_paths_and_ids,
    init_blip2_model,
    extract_dense_captions_blip2
)
from utils.image_utils import load_image


def setup_logging():
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )


def test_blip2_dense_captions(num_images: int = 10):
    """æµ‹è¯•BLIP-2å¯†é›†æè¿°ç”Ÿæˆã€‚
    
    Args:
        num_images: è¦å¤„ç†çš„å›¾åƒæ•°é‡
    """
    setup_logging()
    
    # åŠ è½½é…ç½®
    cfg = load_config()
    
    dense_config = cfg.get("dense_descriptor", {})
    model_path = dense_config.get("model_path", "../models/blip2-opt-2.7b/")
    prompt = dense_config.get("prompt", "Question: List the objects, scenes, and actions in this image with very short phrases. Answer: ")
    max_new_tokens = dense_config.get("max_new_tokens", 100)
    num_beams = dense_config.get("num_beams", 5)
    
    data_config = cfg.get("data_config", {})
    images_dir = data_config.get("coco_images_dir")
    annotations_path = data_config.get("coco_annotations_path")
    
    print("=" * 60)
    print("BLIP-2 å¯†é›†æè¿°ç”Ÿæˆæµ‹è¯•")
    print("=" * 60)
    print(f"æ¨¡å‹è·¯å¾„: {model_path}")
    print(f"æç¤ºè¯: {prompt}")
    print(f"æµ‹è¯•å›¾åƒæ•°é‡: {num_images}")
    print()
    
    # æ£€æŸ¥æ¨¡å‹è·¯å¾„
    if not os.path.exists(model_path):
        print(f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
        print("è¯·å…ˆä¸‹è½½BLIP-2æ¨¡å‹åˆ°è¯¥è·¯å¾„")
        return False
    
    # åŠ è½½COCOå›¾åƒæ˜ å°„
    print("åŠ è½½COCOå›¾åƒæ˜ å°„...")
    image_id_to_filename = load_coco_image_mapping(annotations_path)
    image_ids, image_paths = image_paths_and_ids(images_dir, image_id_to_filename)
    
    if len(image_paths) == 0:
        print(f"âŒ æœªæ‰¾åˆ°å›¾åƒæ–‡ä»¶: {images_dir}")
        return False
    
    print(f"âœ… æ‰¾åˆ° {len(image_paths)} å¼ å›¾åƒ")
    print()
    
    # é™åˆ¶æµ‹è¯•å›¾åƒæ•°é‡
    test_image_ids = image_ids[:num_images]
    test_image_paths = image_paths[:num_images]
    
    # ç¡®å®šè®¾å¤‡
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    print()
    
    # åŠ è½½æ¨¡å‹
    print("æ­£åœ¨åŠ è½½BLIP-2æ¨¡å‹...")
    try:
        processor, model = init_blip2_model(model_path, device=device)
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        print()
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    # å¤„ç†æµ‹è¯•å›¾åƒ
    print("=" * 60)
    print("å¼€å§‹å¤„ç†æµ‹è¯•å›¾åƒ...")
    print("=" * 60)
    print()
    
    results = {}
    
    for i, (image_id, image_path) in enumerate(zip(test_image_ids, test_image_paths), 1):
        print(f"[{i}/{num_images}] å¤„ç†å›¾åƒ ID {image_id}: {os.path.basename(image_path)}")
        
        try:
            phrases = extract_dense_captions_blip2(
                model,
                processor,
                image_path,
                prompt,
                device=device,
                max_new_tokens=max_new_tokens,
                num_beams=num_beams
            )
            
            results[image_id] = phrases
            
            if phrases:
                print(f"  âœ… ç”Ÿæˆäº† {len(phrases)} ä¸ªçŸ­è¯­:")
                for j, phrase in enumerate(phrases[:5], 1):  # åªæ˜¾ç¤ºå‰5ä¸ª
                    print(f"     {j}. {phrase}")
                if len(phrases) > 5:
                    print(f"     ... è¿˜æœ‰ {len(phrases) - 5} ä¸ªçŸ­è¯­")
            else:
                print(f"  âš ï¸  æœªç”ŸæˆçŸ­è¯­")
            
        except Exception as e:
            print(f"  âŒ å¤„ç†å¤±è´¥: {e}")
            results[image_id] = []
        
        print()
    
    # éªŒè¯ç»“æœæ ¼å¼
    print("=" * 60)
    print("éªŒè¯è¾“å‡ºæ ¼å¼")
    print("=" * 60)
    
    all_valid = True
    for image_id, phrases in results.items():
        # æ£€æŸ¥æ˜¯å¦ä¸ºåˆ—è¡¨
        if not isinstance(phrases, list):
            print(f"âŒ å›¾åƒ {image_id}: è¾“å‡ºä¸æ˜¯åˆ—è¡¨ç±»å‹ï¼Œè€Œæ˜¯ {type(phrases)}")
            all_valid = False
            continue
        
        # æ£€æŸ¥åˆ—è¡¨å…ƒç´ æ˜¯å¦ä¸ºå­—ç¬¦ä¸²
        if phrases and not all(isinstance(p, str) for p in phrases):
            print(f"âŒ å›¾åƒ {image_id}: åˆ—è¡¨åŒ…å«éå­—ç¬¦ä¸²å…ƒç´ ")
            all_valid = False
            continue
        
        # æ£€æŸ¥æ˜¯å¦æœ‰éç©ºå­—ç¬¦ä¸²
        non_empty_phrases = [p for p in phrases if p and p.strip()]
        if non_empty_phrases:
            print(f"âœ… å›¾åƒ {image_id}: {len(non_empty_phrases)} ä¸ªæœ‰æ•ˆçŸ­è¯­")
        else:
            print(f"âš ï¸  å›¾åƒ {image_id}: æ²¡æœ‰æœ‰æ•ˆçŸ­è¯­")
    
    print()
    
    # ç»Ÿè®¡ä¿¡æ¯
    images_with_phrases = sum(1 for phrases in results.values() if phrases)
    total_phrases = sum(len(phrases) for phrases in results.values())
    avg_phrases = total_phrases / len(results) if results else 0
    
    print("=" * 60)
    print("æµ‹è¯•æ€»ç»“")
    print("=" * 60)
    print(f"å¤„ç†çš„å›¾åƒæ•°é‡: {len(results)}")
    print(f"åŒ…å«çŸ­è¯­çš„å›¾åƒ: {images_with_phrases}")
    print(f"æ€»çŸ­è¯­æ•°: {total_phrases}")
    print(f"å¹³å‡æ¯å¼ å›¾åƒçŸ­è¯­æ•°: {avg_phrases:.2f}")
    print(f"è¾“å‡ºæ ¼å¼éªŒè¯: {'âœ… é€šè¿‡' if all_valid else 'âŒ å¤±è´¥'}")
    print()
    
    # æ˜¾ç¤ºç¤ºä¾‹è¾“å‡ºæ ¼å¼
    print("=" * 60)
    print("ç¤ºä¾‹è¾“å‡ºæ ¼å¼ (å­—å…¸å½¢å¼)")
    print("=" * 60)
    
    example_output = {k: v for k, v in list(results.items())[:3]}
    for image_id, phrases in example_output.items():
        print(f"  {image_id}: {phrases}")
    
    print()
    
    if all_valid and images_with_phrases > 0:
        print("ğŸ‰ æµ‹è¯•é€šè¿‡ï¼è¾“å‡ºæ ¼å¼æ­£ç¡®ï¼Œå¯ä»¥ç»§ç»­å¤„ç†å…¨éƒ¨å›¾åƒã€‚")
        return True
    else:
        print("âŒ æµ‹è¯•æœªå®Œå…¨é€šè¿‡ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯ã€‚")
        return False


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="æµ‹è¯•BLIP-2å¯†é›†æè¿°ç”Ÿæˆ")
    parser.add_argument(
        "--num-images",
        type=int,
        default=10,
        help="è¦å¤„ç†çš„æµ‹è¯•å›¾åƒæ•°é‡ (é»˜è®¤: 10)"
    )
    
    args = parser.parse_args()
    
    success = test_blip2_dense_captions(num_images=args.num_images)
    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()

