model_config:
  clip_model_path: "../models/clip-vit-base-patch32/"
  llm_model_path: "../models/flan-t5-base/"
  detection_model_path: "../models/DETR/"

data_config:
  coco_images_dir: "/home/m025/qqw/coco/train2017/"
  coco_annotations_path: "/home/m025/qqw/coco/annotations/captions_train2017.json"
  coco_instances_annotations_path: "/home/m025/qqw/coco/annotations/instances_train2017.json"  # COCO物体检测标注文件

retrieval_config:
  top_k: 5

  use_patch_retrieval: true  # 启用分块检索功能
  use_object_aware_retrieval: true  # 启用物体感知混合检索功能
  use_dense_retrieval: false  # 启用密集描述混合检索功能（如果为true，将使用DenseDescriptorHybridRetriever）

# 物体感知混合检索配置
hybrid_retrieval:
  enabled: false  # 是否启用物体感知混合检索（如果为true，将使用ObjectAwareHybridRetriever）
  object_weight: 1  # 物体匹配度在混合分数中的权重（范围0.0-1.0，0.0表示只使用CLIP分数，1.0表示只使用物体匹配度）
  initial_recall_k: 100  # 第一阶段CLIP检索的召回数量（扩大召回池以便重排序）
  dense_object_weight: 0.7  # 密集描述匹配度在混合分数中的权重（用于DenseDescriptorHybridRetriever，范围0.0-1.0）

# 密集描述配置
dense_descriptor:
  model_type: "blip2"  # 模型类型: blip2
  model_path: "../models/blip2-opt-2.7b/"  # 密集描述生成模型路径
  prompt: "Generate a detailed list of phrases describing various elements in this image. Including objects, characters and any entity. Format your response as a list of key points, with each item being a very short phrase. Don't write complete sentences. Answer:"  # 提示词（用于引导生成短语列表）
  knowledge_base_path: "./output/image_id_to_dense_captions.pkl"  # 密集描述知识库路径
  embedding_model_path: "../models/all-MiniLM-L6-v2/"  # 句子嵌入模型路径（用于计算描述相似度）
  batch_size: 8  # 批处理大小（用于进度显示，实际为逐张处理）
  max_new_tokens: 100  # 最大生成token数
  num_beams: 5  # beam search数量
  checkpoint_interval: 100  # 检查点保存间隔（处理的图像数量）

generation_config:
  max_length: 20
  num_beams: 2

runtime_config:
  # 'deploy' -> on generation failure, use fallback composed from retrieved captions
  # 'test'   -> on generation failure, print a failure message
  mode: "test"

knowledge_base_config:
  knowledge_base_path: "./output/coco_knowledge_base.faiss"
  image_id_to_captions_path: "./output/image_id_to_captions.pkl"
  image_id_to_objects_path: "./output/image_id_to_objects.pkl"  # 图像ID到物体类别列表的映射（用于物体感知检索）

log_config:
  log_level: "ERROR"

patch_config:
  # 目标检测配置
  detection_confidence_threshold: 0.7  # 检测置信度阈值
  max_local_regions: 5  # 最大局部区域数量
  expand_ratio: 0.1  # 边界框扩展比例
  
  # 局部检索配置
  local_retrieval_top_k: 1  # 每个局部区域检索的top_k

  # 调试选项
  save_debug_patches: true  # 是否保存检测到的区域图像用于调试

lora_config:
  enabled: false
  weights_path: "lora_training/checkpoints"  # LoRA 适配器路径
  merge_and_unload: false

prompt_tuning:
  enabled: false  # 启用/禁用 Prompt Tuning
  prompt_length: 15  # Prompt tokens 数量
  initialization: "random"  # 初始化方式: "random" 或 "text"
  weights_path: "prompt_tuning/checkpoints/best_model/prompt_embeddings.pt"  # Prompt embeddings 路径

evaluation:
  val_images_dir: "/home/m025/qqw/coco/val2017/"
  val_annotations_path: "/home/m025/qqw/coco/annotations/captions_val2017.json"
  subset_size: null
  output_dir: "./evaluation_results/"
  save_individual_results: true

metrics:
  bleu: true
  rouge: false
  cider: true
  spice: false

description_optimization:
  enabled: true
  embedding_model: "../models/all-MiniLM-L6-v2/"
  max_final_descriptions: 5
  clustering_algorithm: "hdbscan"
  min_cluster_size: 2
  score_weights:
    cluster_similarity: 0.4
    image_similarity: 0.5
    brevity: 0.1
  hdbscan:
    min_samples: 1
    cluster_selection_epsilon: 0.1
