model_config:
  clip_model_path: "../models/clip-vit-base-patch32/"
  llm_model_path: "../models/flan-t5-base/"

data_config:
  coco_images_dir: "/home/m025/qqw/coco/train2017/"
  coco_annotations_path: "/home/m025/qqw/coco/annotations/captions_train2017.json"

retrieval_config:
  top_k: 1
  use_patch_retrieval: true  # 启用分块检索功能

generation_config:
  max_length: 100
  num_beams: 3

runtime_config:
  # 'deploy' -> on generation failure, use fallback composed from retrieved captions
  # 'test'   -> on generation failure, print a failure message
  mode: "test"

knowledge_base_config:
  knowledge_base_path: "./output/coco_knowledge_base.faiss"
  image_id_to_captions_path: "./output/image_id_to_captions.pkl"

log_config:
  log_level: "ERROR"

patch_config:
  # 目标检测配置
  detection_confidence_threshold: 0.7  # 检测置信度阈值
  max_local_regions: 5  # 最大局部区域数量
  expand_ratio: 0.1  # 边界框扩展比例
  
  # 局部检索配置
  local_retrieval_top_k: 1  # 每个局部区域检索的top_k
  
  # 提示词模板配置
  global_prompt_section:   "### Global Context (Overall Scene):\n"
  local_prompt_section: "### Localized Details (Key Regions):\n"
  # final_instruction:  "CRITICAL ANALYSIS INSTRUCTIONS:\n 1. Identify common patterns and recurring elements across ALL descriptions\n 2. Infer the complete scene by synthesizing information from multiple local regions\n 3. Count important elements when similar objects appear in multiple local descriptions\n 4. Generate ONE coherent sentence that captures the entire scene\n 5. DO NOT copy any existing description verbatim - create entirely new phrasing\n 6. Output exactly ONE sentence without bullet points or line breaks\n 7. Focus on what the image actually contains based on all available evidence\n\n Final image description:"
  
  # 调试选项
  save_debug_patches: true  # 是否保存检测到的区域图像用于调试
